{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm_crf_study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from model import NERModel\n",
    "from dataset_loader import DatasetLoader\n",
    "from progressbar import ProgressBar\n",
    "from ner_metrics import SeqEntityScore\n",
    "from data_processor import CluenerProcessor\n",
    "from lr_scheduler import ReduceLROnPlateau\n",
    "from utils_ner import get_entities\n",
    "from common import (init_logger,\n",
    "                    logger,\n",
    "                    json_to_text,\n",
    "                    load_model,\n",
    "                    AverageMeter,\n",
    "                    seed_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.do_train = True\n",
    "        self.do_eval = True \n",
    "        self.do_predict = True\n",
    "        self.markup = \"bios\"     # choices=['bios', 'bio']\n",
    "        self.arch = \"bilstm_crf\"\n",
    "        self.learning_rate = 0.001 \n",
    "        self.seed = 1234\n",
    "        self.gpu = \"\"\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 32\n",
    "        self.embedding_size = 128\n",
    "        self.hidden_size = 384\n",
    "        self.grad_norm = 5.0      # Max gradient norm\n",
    "        self.task_name =\"ner\"\n",
    "        self.data_dir = Path(\"./dataset/cluener\")\n",
    "        self.train_path = self.data_dir / 'train.json'\n",
    "        self.dev_path =self.data_dir / 'dev.json'\n",
    "        self.test_path = self.data_dir / 'test.json'\n",
    "        self.output_dir = Path(\"./outputs\")\n",
    "\n",
    "        self.label2id = {\n",
    "            \"O\": 0,\n",
    "            \"B-address\":1,\n",
    "            \"B-book\":2,\n",
    "            \"B-company\":3,\n",
    "            'B-game':4,\n",
    "            'B-government':5,\n",
    "            'B-movie':6,\n",
    "            'B-name':7,\n",
    "            'B-organization':8,\n",
    "            'B-position':9,\n",
    "            'B-scene':10,\n",
    "            \"I-address\":11,\n",
    "            \"I-book\":12,\n",
    "            \"I-company\":13,\n",
    "            'I-game':14,\n",
    "            'I-government':15,\n",
    "            'I-movie':16,\n",
    "            'I-name':17,\n",
    "            'I-organization':18,\n",
    "            'I-position':19,\n",
    "            'I-scene':20,\n",
    "            \"S-address\":21,\n",
    "            \"S-book\":22,\n",
    "            \"S-company\":23,\n",
    "            'S-game':24,\n",
    "            'S-government':25,\n",
    "            'S-movie':26,\n",
    "            'S-name':27,\n",
    "            'S-organization':28,\n",
    "            'S-position':29,\n",
    "            'S-scene':30,\n",
    "            \"<START>\": 31,\n",
    "            \"<STOP>\": 32\n",
    "        }\n",
    "        \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 文件夹创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not args.output_dir.exists():\n",
    "    args.output_dir.mkdir()\n",
    "args.output_dir = args.output_dir / '{}'.format(args.arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not args.output_dir.exists():\n",
    "    args.output_dir.mkdir()\n",
    "init_logger(log_file=str(args.output_dir / '{}-{}.log'.format(args.arch, args.task_name)))\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if args.gpu!='':\n",
    "    args.device = torch.device(f\"cuda:{args.gpu}\")\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### label -> id 的 映射表构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args.id2label = {i: label for i, label in enumerate(args.label2id)}\n",
    "args.label2id = args.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 数据处理类 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processor = CluenerProcessor(data_dir=config.data_dir)\n",
    "processor.get_vocab()                            # 构建词典，并保存到 vocab.pkl 文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = NERModel(\n",
    "    vocab_size=len(processor.vocab), \n",
    "    embedding_size=args.embedding_size,\n",
    "    hidden_size=args.hidden_size,\n",
    "    device=args.device,\n",
    "    label2id=args.label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERModel(\n",
       "  (embedding): Embedding(3821, 128)\n",
       "  (bilstm): LSTM(128, 384, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (dropout): SpatialDropout(p=0.1, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=33, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载 训练 和 验证 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```s\n",
    "[\n",
    "    {\n",
    "        'id': 'train_0',\n",
    "        'context': '浙 商 银 行 企 业 信 贷 部 叶 老 桂 博 士 ...',\n",
    "        'tag': 'B-company I-company I-company I-company O O O O O B-name I-name I-name ...',\n",
    "         'raw_context': '浙商银行企业信贷部叶老桂博士...'\n",
    "     }, ...\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     1,
     6,
     10
    ]
   },
   "outputs": [],
   "source": [
    "# 功能：从 cache 或 数据文件中加载 训练或验证数据\n",
    "def load_and_cache_examples(args,processor, data_type='train'):\n",
    "    '''\n",
    "        功能：从 cache 或 数据文件中加载 训练或验证数据\n",
    "    '''\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_examples_file = args.data_dir / 'cached_crf-{}_{}_{}'.format(\n",
    "        data_type,\n",
    "        args.arch,\n",
    "        str(args.task_name))\n",
    "    if cached_examples_file.exists():\n",
    "        logger.info(\"Loading features from cached file %s\", cached_examples_file)\n",
    "        examples = torch.load(cached_examples_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        if data_type == 'train':\n",
    "            examples = processor.get_train_examples()\n",
    "        elif data_type == 'dev':\n",
    "            examples = processor.get_dev_examples()\n",
    "        logger.info(\"Saving features into cached file %s\", cached_examples_file)\n",
    "        torch.save(examples, str(cached_examples_file))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args,model,processor):\n",
    "    # step 1：加载验证集\n",
    "    eval_dataset = load_and_cache_examples(args,processor, data_type='dev')\n",
    "    # step 2：定义 DatasetLoader 对象。并对数据 token 和 label 转化为 id，和按 batch_size 大小切割\n",
    "    eval_dataloader = DatasetLoader(\n",
    "        data=eval_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False, \n",
    "        seed=args.seed, \n",
    "        sort=False,\n",
    "        vocab=processor.vocab, \n",
    "        label2id=args.label2id)\n",
    "    '''进度条'''\n",
    "    pbar = ProgressBar(n_total=len(eval_dataloader), desc=\"Evaluating\")\n",
    "    '''评价指标定义'''\n",
    "    metric = SeqEntityScore(args.id2label,markup=args.markup)\n",
    "    '''计算并存储平均值和当前值'''\n",
    "    eval_loss = AverageMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            input_ids, input_mask, input_tags, input_lens = batch\n",
    "            input_ids = input_ids.to(args.device)\n",
    "            input_mask = input_mask.to(args.device)\n",
    "            input_tags = input_tags.to(args.device)\n",
    "            features, loss = model.forward_loss(input_ids, input_mask, input_lens, input_tags)\n",
    "            eval_loss.update(val=loss.item(), n=input_ids.size(0))\n",
    "            '''利用 CRF 解码 标注序列'''\n",
    "            tags, _ = model.crf._obtain_labels(features, args.id2label, input_lens)\n",
    "            input_tags = input_tags.cpu().numpy()\n",
    "            target = [input_[:len_] for input_, len_ in zip(input_tags, input_lens)]\n",
    "            metric.update(pred_paths=tags, label_paths=target)\n",
    "            pbar(step=step)\n",
    "    print(\" \")\n",
    "    eval_info, class_info = metric.result()\n",
    "    eval_info = {f'eval_{key}': value for key, value in eval_info.items()}\n",
    "    result = {'eval_loss': eval_loss.avg}\n",
    "    result = dict(result, **eval_info)\n",
    "    return result, class_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(args,model,processor):\n",
    "    # step 1：加载 训练集数据\n",
    "    train_dataset = load_and_cache_examples(args, processor, data_type='train')\n",
    "    # step 2：定义 DatasetLoader 对象。并对数据 token 和 label 转化为 id，和按 batch_size 大小切割\n",
    "    train_loader = DatasetLoader(\n",
    "            data=train_dataset, \n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False, seed=args.seed, sort=True,\n",
    "            vocab = processor.vocab,label2id = args.label2id\n",
    "        )\n",
    "    # step 3：定义优化函数\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    '''\n",
    "        作用：当指标停止改善时，降低学习率。\n",
    "        一旦学习停滞，模型通常会受益于将学习率降低2-10倍。\n",
    "        该调度程序读取度量标准数量，如果没有发现“耐心”时期的改善，则会降低学习率。\n",
    "    '''\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='max', \n",
    "        factor=0.5, patience=3,\n",
    "        verbose=1, epsilon=1e-4, \n",
    "        cooldown=0, min_lr=0, eps=1e-8)\n",
    "    # step 4：模型训练\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, 1 + args.epochs):\n",
    "        print(f\"Epoch {epoch}/{args.epochs}\")\n",
    "        '''进度条'''\n",
    "        pbar = ProgressBar(n_total=len(train_loader), desc='Training')\n",
    "        '''计算并存储平均值和当前值'''\n",
    "        train_loss = AverageMeter()\n",
    "        model.train()\n",
    "        assert model.training\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids, input_mask, input_tags, input_lens = batch\n",
    "            input_ids = input_ids.to(args.device)\n",
    "            input_mask = input_mask.to(args.device)\n",
    "            input_tags = input_tags.to(args.device)\n",
    "            features, loss = model.forward_loss(input_ids, input_mask, input_lens, input_tags)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar(step=step, info={'loss': loss.item()})\n",
    "            train_loss.update(loss.item(), n=1)\n",
    "        print(\" \")\n",
    "        train_log = {'loss': train_loss.avg}\n",
    "        if 'cuda' in str(args.device):\n",
    "            torch.cuda.empty_cache()\n",
    "        # step 5：评估 训练效果\n",
    "        eval_log, class_info = evaluate(args,model,processor)\n",
    "        logs = dict(train_log, **eval_log)\n",
    "        show_info = f'\\nEpoch: {epoch} - ' + \"-\".join([f' {key}: {value:.4f} ' for key, value in logs.items()])\n",
    "        logger.info(show_info)\n",
    "        scheduler.epoch_step(logs['eval_f1'], epoch)\n",
    "        # step 6：模型保存\n",
    "        if logs['eval_f1'] > best_f1:\n",
    "            logger.info(f\"\\nEpoch {epoch}: eval_f1 improved from {best_f1} to {logs['eval_f1']}\")\n",
    "            logger.info(\"save model to disk.\")\n",
    "            best_f1 = logs['eval_f1']\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                model_stat_dict = model.module.state_dict()\n",
    "            else:\n",
    "                model_stat_dict = model.state_dict()\n",
    "            state = {'epoch': epoch, 'arch': args.arch, 'state_dict': model_stat_dict}\n",
    "            model_path = args.output_dir / 'best-model.bin'\n",
    "            torch.save(state, str(model_path))\n",
    "            print(\"Eval Entity Score: \")\n",
    "            for key, value in class_info.items():\n",
    "                info = f\"Subject: {key} - Acc: {value['acc']} - Recall: {value['recall']} - F1: {value['f1']}\"\n",
    "                logger.info(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2020 15:04:08 - INFO - root -   Loading features from cached file dataset\\cluener\\cached_crf-train_bilstm_crf_ner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 batches created\n",
      "Epoch 1/50\n",
      "[Training] 334/336 [============================>.] - ETA: 1s  loss: 1.6212    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2020 15:09:17 - INFO - root -   Loading features from cached file dataset\\cluener\\cached_crf-dev_bilstm_crf_ner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 336/336 [==============================] 918.7ms/step  loss: 3.8900  \n",
      "42 batches created\n",
      "[Evaluating] 41/42 [============================>.] - ETA: 0ss"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2020 15:09:41 - INFO - root -   \n",
      "Epoch: 1 -  loss: 5.8060 - eval_loss: 10.4961 - eval_acc: 0.5807 - eval_recall: 0.6263 - eval_f1: 0.6027 \n",
      "12/27/2020 15:09:41 - INFO - root -   \n",
      "Epoch 1: eval_f1 improved from 0 to 0.602662490211433\n",
      "12/27/2020 15:09:41 - INFO - root -   save model to disk.\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: name - Acc: 0.5699 - Recall: 0.6925 - F1: 0.6252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Evaluating] 42/42 [==============================] 578.7ms/step \n",
      "Eval Entity Score: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2020 15:09:41 - INFO - root -   Subject: address - Acc: 0.4749 - Recall: 0.3298 - F1: 0.3892\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: movie - Acc: 0.604 - Recall: 0.596 - F1: 0.6\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: position - Acc: 0.6932 - Recall: 0.7252 - F1: 0.7088\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: organization - Acc: 0.6078 - Recall: 0.7221 - F1: 0.66\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: company - Acc: 0.6385 - Recall: 0.6402 - F1: 0.6394\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: scene - Acc: 0.3861 - Recall: 0.5598 - F1: 0.457\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: government - Acc: 0.6726 - Recall: 0.6073 - F1: 0.6383\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: book - Acc: 0.7128 - Recall: 0.4351 - F1: 0.5403\n",
      "12/27/2020 15:09:41 - INFO - root -   Subject: game - Acc: 0.5177 - Recall: 0.7932 - F1: 0.6265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "[Training] 271/336 [=======================>......] - ETA: 1:04  loss: 2.8344  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0cdd4050d3b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-4bdd559e5bbd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, model, processor)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0minput_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\progrom\\python\\python\\python3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\progrom\\python\\python\\python3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    train(args,model,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2020 14:18:19 - INFO - root -   Loading features from cached file dataset\\cluener\\cached_crf-train_bilstm_crf_ner\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(args, processor, data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
