{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本分类任务学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from importlib import import_module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from utils import get_time_dif\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n",
    "        self.model = \"TextCNN\"\n",
    "        # 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz, 随机初始化:random\n",
    "        self.embedding = \"embedding_SougouNews.npz\"  \n",
    "        self.word = False\n",
    "        self.dataset = 'THUCNews'  # 数据集\n",
    "        \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断 是不是 调用 FastText 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.model == 'FastText':\n",
    "    from utils_fasttext import build_dataset, build_iterator, get_time_dif\n",
    "    args.embedding = 'random'\n",
    "else:\n",
    "    from utils import build_dataset, build_iterator, get_time_dif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = import_module('models.' + args.model)\n",
    "config = x.Config(args.dataset, args.embedding)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载 数据，并构建 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4815it [00:00, 48086.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocab size: 4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:03, 48948.33it/s]\n",
      "10000it [00:00, 41161.34it/s]\n",
      "10000it [00:00, 49952.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading data...\")\n",
    "vocab, train_data, dev_data, test_data = build_dataset(config, args.word)\n",
    "train_data = train_data[:int(0.5*len(train_data))]\n",
    "train_iter = build_iterator(train_data, config)\n",
    "dev_iter = build_iterator(dev_data, config)\n",
    "test_iter = build_iterator(test_data, config)\n",
    "time_dif = get_time_dif(start_time)\n",
    "print(\"Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 权重初始化，默认xavier\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.n_vocab = len(vocab)\n",
    "model = x.Model(config).to(config.device)\n",
    "if args.model != 'Transformer':\n",
    "    init_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, model, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            loss = model(trains,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(model.outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "    test(config, model, test_iter)\n",
    "\n",
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            loss = model(texts, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(model.outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (embedding): Embedding(4762, 300)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")>\n",
      "Epoch [1/20]\n",
      "Iter:      0,  Train Loss:   2.3,  Train Acc: 20.31%,  Val Loss:   2.3,  Val Acc: 10.20%,  Time: 0:00:12 *\n",
      "Iter:    100,  Train Loss:  0.51,  Train Acc: 85.16%,  Val Loss:  0.68,  Val Acc: 78.81%,  Time: 0:01:32 *\n",
      "Iter:    200,  Train Loss:  0.52,  Train Acc: 80.47%,  Val Loss:  0.54,  Val Acc: 83.54%,  Time: 0:02:53 *\n",
      "Iter:    300,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:   0.5,  Val Acc: 84.11%,  Time: 0:04:09 *\n",
      "Iter:    400,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 85.67%,  Time: 0:05:24 *\n",
      "Iter:    500,  Train Loss:  0.31,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 86.62%,  Time: 0:06:43 *\n",
      "Iter:    600,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.42,  Val Acc: 87.15%,  Time: 0:07:58 *\n",
      "Iter:    700,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 87.07%,  Time: 0:09:20 *\n",
      "Epoch [2/20]\n",
      "Iter:    800,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.41,  Val Acc: 87.70%,  Time: 0:10:41 *\n",
      "Iter:    900,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 87.88%,  Time: 0:12:00 *\n",
      "Iter:   1000,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 88.06%,  Time: 0:12:38 *\n",
      "Iter:   1100,  Train Loss:  0.37,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 88.03%,  Time: 0:13:15 \n",
      "Iter:   1200,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.27%,  Time: 0:13:52 *\n",
      "Iter:   1300,  Train Loss:  0.53,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 88.68%,  Time: 0:14:28 *\n",
      "Iter:   1400,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 88.84%,  Time: 0:15:04 *\n",
      "Epoch [3/20]\n",
      "Iter:   1500,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.01%,  Time: 0:15:40 \n",
      "Iter:   1600,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.07%,  Time: 0:16:17 *\n",
      "Iter:   1700,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 89.31%,  Time: 0:16:54 *\n",
      "Iter:   1800,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.38,  Val Acc: 88.52%,  Time: 0:17:31 \n",
      "Iter:   1900,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.39%,  Time: 0:18:09 \n",
      "Iter:   2000,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 89.08%,  Time: 0:18:45 *\n",
      "Iter:   2100,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 89.01%,  Time: 0:19:22 \n",
      "Epoch [4/20]\n",
      "Iter:   2200,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 89.49%,  Time: 0:19:58 *\n",
      "Iter:   2300,  Train Loss:  0.25,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.34%,  Time: 0:20:35 \n",
      "Iter:   2400,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.36,  Val Acc: 89.30%,  Time: 0:21:11 \n",
      "Iter:   2500,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.02%,  Time: 0:21:47 \n",
      "Iter:   2600,  Train Loss:  0.14,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 89.56%,  Time: 0:22:23 \n",
      "Iter:   2700,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 89.68%,  Time: 0:23:00 *\n",
      "Iter:   2800,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 89.55%,  Time: 0:23:39 \n",
      "Epoch [5/20]\n",
      "Iter:   2900,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 89.46%,  Time: 0:24:16 \n",
      "Iter:   3000,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.36%,  Time: 0:24:54 \n",
      "Iter:   3100,  Train Loss:  0.15,  Train Acc: 96.88%,  Val Loss:  0.37,  Val Acc: 89.10%,  Time: 0:25:31 \n",
      "Iter:   3200,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.38,  Val Acc: 89.30%,  Time: 0:26:08 \n",
      "Iter:   3300,  Train Loss:  0.15,  Train Acc: 94.53%,  Val Loss:  0.37,  Val Acc: 89.27%,  Time: 0:26:46 \n",
      "Iter:   3400,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.38,  Val Acc: 89.36%,  Time: 0:27:23 \n",
      "Iter:   3500,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.38,  Val Acc: 89.26%,  Time: 0:28:00 \n",
      "Epoch [6/20]\n",
      "Iter:   3600,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.39,  Val Acc: 89.12%,  Time: 0:28:36 \n",
      "Iter:   3700,  Train Loss:  0.15,  Train Acc: 94.53%,  Val Loss:  0.38,  Val Acc: 89.56%,  Time: 0:29:12 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.34,  Test Acc: 89.92%\n",
      "Precision, Recall and F1-Score...\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      finance     0.9139    0.8700    0.8914      1000\n",
      "       realty     0.9134    0.9280    0.9206      1000\n",
      "       stocks     0.8432    0.8440    0.8436      1000\n",
      "    education     0.9209    0.9550    0.9377      1000\n",
      "      science     0.8727    0.8430    0.8576      1000\n",
      "      society     0.8955    0.9000    0.8978      1000\n",
      "     politics     0.8682    0.8830    0.8756      1000\n",
      "       sports     0.9527    0.9460    0.9493      1000\n",
      "         game     0.9172    0.9080    0.9126      1000\n",
      "entertainment     0.8944    0.9150    0.9046      1000\n",
      "\n",
      "  avg / total     0.8992    0.8992    0.8991     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[870  16  65   8   9   6  12   9   0   5]\n",
      " [ 14 928  13   3   4  14  11   3   1   9]\n",
      " [ 47  27 844   4  30   2  36   3   6   1]\n",
      " [  0   3   2 955   6   9   5   2   4  14]\n",
      " [  2   7  38   8 843  19  27   2  37  17]\n",
      " [  3  16   4  24   5 900  27   2   4  15]\n",
      " [  9  11  21  15  19  30 883   2   1   9]\n",
      " [  1   2   5   2   3   7   4 946   7  23]\n",
      " [  3   2   5   7  37   6   6  11 908  15]\n",
      " [  3   4   4  11  10  12   6  13  22 915]]\n",
      "Time usage: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "train(config, model, train_iter, dev_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
