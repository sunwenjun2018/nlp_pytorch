{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本分类任务学习【加入对抗生成】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "1. [【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现](https://zhuanlan.zhihu.com/p/91269728)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from importlib import import_module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from utils import get_time_dif\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n",
    "        self.model = \"TextCNN\"\n",
    "        # 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz, 随机初始化:random\n",
    "        self.embedding = \"embedding_SougouNews.npz\"  \n",
    "        self.word = False\n",
    "        self.dataset = 'THUCNews'  # 数据集\n",
    "        \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断 是不是 调用 FastText 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.model == 'FastText':\n",
    "    from utils_fasttext import build_dataset, build_iterator, get_time_dif\n",
    "    args.embedding = 'random'\n",
    "else:\n",
    "    from utils import build_dataset, build_iterator, get_time_dif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = import_module('models.' + args.model)\n",
    "config = x.Config(args.dataset, args.embedding)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载 数据，并构建 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2782it [00:00, 27618.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocab size: 4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:07, 24505.27it/s]\n",
      "10000it [00:00, 23703.74it/s]\n",
      "10000it [00:00, 26043.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading data...\")\n",
    "vocab, train_data, dev_data, test_data = build_dataset(config, args.word)\n",
    "train_data = train_data[:int(0.5*len(train_data))]\n",
    "train_iter = build_iterator(train_data, config)\n",
    "dev_iter = build_iterator(dev_data, config)\n",
    "test_iter = build_iterator(test_data, config)\n",
    "time_dif = get_time_dif(start_time)\n",
    "print(\"Time usage:\", time_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 权重初始化，默认xavier\n",
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.n_vocab = len(vocab)\n",
    "model = x.Model(config).to(config.device)\n",
    "if args.model != 'Transformer':\n",
    "    init_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 模型训练 【使用 FGM 做对抗生成】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 对抗生成 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from adversarial_training.FGM import FGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 方法编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(config, model, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    # 对抗生成 初始化\n",
    "    fgm = FGM(model)\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            loss  = model(trains,labels)\n",
    "            loss.backward()   # 反向传播，得到正常的grad\n",
    "            \n",
    "            # 对抗训练\n",
    "            fgm.attack() # 在embedding上添加对抗扰动\n",
    "            loss_adv = model(trains,labels)\n",
    "            loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            fgm.restore() # 恢复embedding参数\n",
    "            \n",
    "            # 梯度下降，更新参数\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(model.outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "    test(config, model, test_iter)\n",
    "\n",
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            loss = model(texts, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(model.outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (embedding): Embedding(4762, 300)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")>\n",
      "Epoch [1/20]\n",
      "Iter:      0,  Train Loss:   2.3,  Train Acc:  7.03%,  Val Loss:   2.6,  Val Acc: 10.73%,  Time: 0:00:07 *\n",
      "Iter:    100,  Train Loss:  0.76,  Train Acc: 73.44%,  Val Loss:  0.69,  Val Acc: 78.37%,  Time: 0:01:17 *\n",
      "Iter:    200,  Train Loss:  0.68,  Train Acc: 77.34%,  Val Loss:  0.55,  Val Acc: 83.13%,  Time: 0:02:27 *\n",
      "Iter:    300,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 84.52%,  Time: 0:03:36 *\n",
      "Iter:    400,  Train Loss:  0.68,  Train Acc: 79.69%,  Val Loss:  0.46,  Val Acc: 85.63%,  Time: 0:04:42 *\n",
      "Iter:    500,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.43,  Val Acc: 86.52%,  Time: 0:05:49 *\n",
      "Iter:    600,  Train Loss:  0.52,  Train Acc: 82.03%,  Val Loss:  0.43,  Val Acc: 86.66%,  Time: 0:06:56 *\n",
      "Iter:    700,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 87.51%,  Time: 0:08:03 *\n",
      "Epoch [2/20]\n",
      "Iter:    800,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:   0.4,  Val Acc: 87.67%,  Time: 0:09:11 *\n",
      "Iter:    900,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 87.75%,  Time: 0:10:18 *\n",
      "Iter:   1000,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 87.90%,  Time: 0:11:27 *\n",
      "Iter:   1100,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 88.18%,  Time: 0:12:36 *\n",
      "Iter:   1200,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.14%,  Time: 0:13:45 *\n",
      "Iter:   1300,  Train Loss:  0.28,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.39%,  Time: 0:14:54 *\n",
      "Iter:   1400,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 88.47%,  Time: 0:16:02 *\n",
      "Epoch [3/20]\n",
      "Iter:   1500,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.02%,  Time: 0:17:09 *\n",
      "Iter:   1600,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 89.15%,  Time: 0:18:17 *\n",
      "Iter:   1700,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 88.81%,  Time: 0:19:26 \n",
      "Iter:   1800,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 88.82%,  Time: 0:20:34 \n",
      "Iter:   1900,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.35,  Val Acc: 89.19%,  Time: 0:21:44 *\n",
      "Iter:   2000,  Train Loss:   0.4,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.15%,  Time: 0:22:51 \n",
      "Iter:   2100,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.05%,  Time: 0:23:58 \n",
      "Epoch [4/20]\n",
      "Iter:   2200,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.32%,  Time: 0:25:04 *\n",
      "Iter:   2300,  Train Loss:  0.24,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 89.18%,  Time: 0:26:09 \n",
      "Iter:   2400,  Train Loss:   0.1,  Train Acc: 93.75%,  Val Loss:  0.36,  Val Acc: 89.10%,  Time: 0:27:14 \n",
      "Iter:   2500,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:28:19 \n",
      "Iter:   2600,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 89.48%,  Time: 0:29:24 \n",
      "Iter:   2700,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.37,  Val Acc: 89.20%,  Time: 0:30:32 \n",
      "Iter:   2800,  Train Loss:  0.23,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 89.15%,  Time: 0:31:39 \n",
      "Epoch [5/20]\n",
      "Iter:   2900,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.38,  Val Acc: 88.87%,  Time: 0:32:45 \n",
      "Iter:   3000,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 89.38%,  Time: 0:33:52 \n",
      "Iter:   3100,  Train Loss:  0.31,  Train Acc: 94.53%,  Val Loss:  0.38,  Val Acc: 89.04%,  Time: 0:34:59 \n",
      "Iter:   3200,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.42%,  Time: 0:36:03 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.34,  Test Acc: 89.73%\n",
      "Precision, Recall and F1-Score...\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      finance     0.9211    0.8750    0.8974      1000\n",
      "       realty     0.8993    0.9290    0.9139      1000\n",
      "       stocks     0.8617    0.8290    0.8451      1000\n",
      "    education     0.9332    0.9500    0.9415      1000\n",
      "      science     0.8446    0.8530    0.8488      1000\n",
      "      society     0.8913    0.9020    0.8966      1000\n",
      "     politics     0.8936    0.8730    0.8832      1000\n",
      "       sports     0.9270    0.9530    0.9398      1000\n",
      "         game     0.8822    0.9140    0.8978      1000\n",
      "entertainment     0.9189    0.8950    0.9068      1000\n",
      "\n",
      "  avg / total     0.8973    0.8973    0.8971     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[875  18  53   4  11  12   7  10   7   3]\n",
      " [ 12 929  14   3   6  10   8   5   5   8]\n",
      " [ 40  31 829   2  47   3  33   5   8   2]\n",
      " [  0   2   2 950   7  11   6   5   4  13]\n",
      " [  5   8  29  10 853  14  17   2  52  10]\n",
      " [  2  21   5  22   9 902  20   4   6   9]\n",
      " [ 10  13  22  10  18  35 873   7   1  11]\n",
      " [  2   2   3   3   5   8   3 953   7  14]\n",
      " [  1   3   5   4  44   5   4  11 914   9]\n",
      " [  3   6   0  10  10  12   6  26  32 895]]\n",
      "Time usage: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "train(config, model, train_iter, dev_iter, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练 【使用 PGD 做对抗生成】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PGC 方法导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adversarial_training.PGD import PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, model, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    # 对抗生成 初始化\n",
    "    pgd = PGD(model)\n",
    "    K = 3\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            loss  = model(trains,labels)\n",
    "            loss.backward()   # 反向传播，得到正常的grad\n",
    "            \n",
    "            # 对抗训练\n",
    "            for t in range(K):\n",
    "                pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.data\n",
    "                if t != K-1:\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    pgd.restore_grad()\n",
    "                loss_adv = model(trains,labels)\n",
    "                loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            pgd.restore() # 恢复embedding参数\n",
    "            \n",
    "            # 梯度下降，更新参数\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(model.outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "    test(config, model, test_iter)\n",
    "\n",
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            loss = model(texts, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(model.outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Model(\n",
      "  (embedding): Embedding(4762, 300)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")>\n",
      "Epoch [1/20]\n",
      "Iter:      0,  Train Loss:   2.3,  Train Acc:  8.59%,  Val Loss:   2.6,  Val Acc: 15.34%,  Time: 0:00:11 *\n",
      "Iter:    100,  Train Loss:  0.71,  Train Acc: 73.44%,  Val Loss:   0.7,  Val Acc: 78.23%,  Time: 0:02:35 *\n",
      "Iter:    200,  Train Loss:  0.69,  Train Acc: 77.34%,  Val Loss:  0.55,  Val Acc: 83.47%,  Time: 0:05:02 *\n",
      "Iter:    300,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:   0.5,  Val Acc: 84.61%,  Time: 0:07:29 *\n",
      "Iter:    400,  Train Loss:  0.72,  Train Acc: 78.91%,  Val Loss:  0.46,  Val Acc: 85.69%,  Time: 0:09:56 *\n",
      "Iter:    500,  Train Loss:  0.42,  Train Acc: 89.84%,  Val Loss:  0.44,  Val Acc: 86.47%,  Time: 0:12:23 *\n",
      "Iter:    600,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 87.04%,  Time: 0:14:48 *\n",
      "Iter:    700,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:   0.4,  Val Acc: 87.16%,  Time: 0:17:14 *\n",
      "Epoch [2/20]\n",
      "Iter:    800,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 87.61%,  Time: 0:19:40 *\n",
      "Iter:    900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 87.90%,  Time: 0:22:08 *\n",
      "Iter:   1000,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 88.02%,  Time: 0:24:36 *\n",
      "Iter:   1100,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 88.20%,  Time: 0:27:04 *\n",
      "Iter:   1200,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 88.12%,  Time: 0:29:31 *\n",
      "Iter:   1300,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.51%,  Time: 0:40:21 *\n",
      "Iter:   1400,  Train Loss:  0.32,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.66%,  Time: 0:42:54 *\n",
      "Epoch [3/20]\n",
      "Iter:   1500,  Train Loss:  0.28,  Train Acc: 92.97%,  Val Loss:  0.36,  Val Acc: 88.86%,  Time: 0:45:46 *\n",
      "Iter:   1600,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.69%,  Time: 0:48:19 \n",
      "Iter:   1700,  Train Loss:  0.23,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.00%,  Time: 0:50:31 *\n",
      "Iter:   1800,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 88.29%,  Time: 0:52:46 \n",
      "Iter:   1900,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 89.05%,  Time: 0:55:12 *\n",
      "Iter:   2000,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.36,  Val Acc: 88.87%,  Time: 0:57:27 *\n",
      "Iter:   2100,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.02%,  Time: 0:59:40 \n",
      "Epoch [4/20]\n",
      "Iter:   2200,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.33%,  Time: 1:01:51 *\n",
      "Iter:   2300,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.35,  Val Acc: 89.14%,  Time: 1:04:03 \n",
      "Iter:   2400,  Train Loss:   0.1,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 88.90%,  Time: 1:06:16 \n",
      "Iter:   2500,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.36,  Val Acc: 89.07%,  Time: 1:08:31 \n",
      "Iter:   2600,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.36,  Val Acc: 89.34%,  Time: 1:10:38 \n",
      "Iter:   2700,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.26%,  Time: 1:12:40 \n",
      "Iter:   2800,  Train Loss:  0.16,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.36%,  Time: 1:14:42 \n",
      "Epoch [5/20]\n",
      "Iter:   2900,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.02%,  Time: 1:16:43 \n",
      "Iter:   3000,  Train Loss:  0.36,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.21%,  Time: 1:18:45 \n",
      "Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.03%,  Time: 1:20:47 \n",
      "Iter:   3200,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.10%,  Time: 1:22:49 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.34,  Test Acc: 89.85%\n",
      "Precision, Recall and F1-Score...\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      finance     0.9022    0.8950    0.8986      1000\n",
      "       realty     0.9029    0.9300    0.9163      1000\n",
      "       stocks     0.8868    0.7910    0.8362      1000\n",
      "    education     0.9505    0.9410    0.9457      1000\n",
      "      science     0.8270    0.8750    0.8503      1000\n",
      "      society     0.8878    0.9100    0.8988      1000\n",
      "     politics     0.8934    0.8800    0.8866      1000\n",
      "       sports     0.9322    0.9480    0.9400      1000\n",
      "         game     0.8994    0.9120    0.9057      1000\n",
      "entertainment     0.9057    0.9030    0.9044      1000\n",
      "\n",
      "  avg / total     0.8988    0.8985    0.8983     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[895  18  42   3  16   7   6   8   3   2]\n",
      " [ 11 930  14   1   3  14   8   5   4  10]\n",
      " [ 57  32 791   4  57   5  36   6  11   1]\n",
      " [  1   2   2 941   9  16   9   4   2  14]\n",
      " [  1  11  20   3 875  17  18   5  37  13]\n",
      " [  6  17   0  16  10 910  18   3   5  15]\n",
      " [ 14   9  16   8  19  33 880   9   1  11]\n",
      " [  3   2   2   3   6   8   3 948   8  17]\n",
      " [  1   0   5   4  50   5   1  11 912  11]\n",
      " [  3   9   0   7  13  10   6  18  31 903]]\n",
      "Time usage: 0:00:06\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "train(config, model, train_iter, dev_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
